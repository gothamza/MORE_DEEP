{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Libraries for general purpose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Text cleaning\n",
    "import re, string\n",
    "# import emoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#PyTorch LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Tokenization for LSTM\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "#set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cyberbullying_tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47692 entries, 0 to 47691\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   tweet_text          47692 non-null  object\n",
      " 1   cyberbullying_type  47692 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 745.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "religion               7997\n",
       "age                    7992\n",
       "ethnicity              7959\n",
       "gender                 7948\n",
       "not_cyberbullying      7937\n",
       "other_cyberbullying    7823\n",
       "Name: cyberbullying_type, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Remove punctuations, links, stopwords, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text): \n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    text = [word for word in text.split() if word not in []]\n",
    "    text = ' '.join(text)\n",
    "    text =' '.join(word for word in text.split() if len(word) < 14) # remove words longer than 14 characters\n",
    "    return text\n",
    "\n",
    "#remove contractions\n",
    "def decontract(text):\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    return text\n",
    "\n",
    "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the \"#\" symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "#Filter special characters such as \"&\" and \"$\" present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "#Remove multiple sequential spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "#Stemming\n",
    "def stemmer(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    ps = PorterStemmer()\n",
    "    return ' '.join([ps.stem(words) for words in tokenized])\n",
    "\n",
    "#Lemmatization \n",
    "#NOTE:Stemming seems to work better for this dataset\n",
    "def lemmatize(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    lm = WordNetLemmatizer()\n",
    "    return ' '.join([lm.lemmatize(words) for words in tokenized])\n",
    "\n",
    "#Then we apply all the defined functions in the following order\n",
    "def deep_clean(text):\n",
    "    text = decontract(text)\n",
    "    text = strip_all_entities(text)\n",
    "    text = clean_hashtags(text)\n",
    "    text = filter_chars(text)\n",
    "    text = remove_mult_spaces(text)\n",
    "    # text = stemmer(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new = []\n",
    "for t in df.tweet_text:\n",
    "    texts_new.append(deep_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = texts_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>in other words katandandre your food was crapi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>why is aussietv so white mkr theblock today su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>a classy whore or more red velvet cupcakes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>meh p thanks for the heads up but not too conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>this is an isis account pretending to be a kur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "                                          text_clean  \n",
       "0  in other words katandandre your food was crapi...  \n",
       "1  why is aussietv so white mkr theblock today su...  \n",
       "2         a classy whore or more red velvet cupcakes  \n",
       "3  meh p thanks for the heads up but not too conc...  \n",
       "4  this is an isis account pretending to be a kur...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2695"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_clean\"].duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"text_clean\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "religion               7955\n",
       "age                    7933\n",
       "ethnicity              7807\n",
       "not_cyberbullying      7718\n",
       "gender                 7642\n",
       "other_cyberbullying    5906\n",
       "Name: cyberbullying_type, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cyberbullying_type.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"cyberbullying_type\"]!=\"other_cyberbullying\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [\"religion\",\"age\",\"ethnicity\",\"gender\",\"not bullying\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets length analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = []\n",
    "for text in df.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_len'] = text_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text_len'] > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44035</th>\n",
       "      <td>You so black and white trying to live like a n...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>you so black and white trying to live like a n...</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>@EurekAlertAAAS: Researchers push to import to...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>researchers push to import top antibullying pr...</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45165</th>\n",
       "      <td>@hermdiggz: “@tayyoung_: FUCK OBAMA, dumb ass ...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>fuck obama dumb ass nigger this bitch ltthis w...</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33724</th>\n",
       "      <td>... I don't feel guilty for killing him, I jus...</td>\n",
       "      <td>age</td>\n",
       "      <td>i do not feel guilty for killing him i just fe...</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>don't make rape jokes!!! don't make gay jokes!...</td>\n",
       "      <td>gender</td>\n",
       "      <td>do not make rape jokes do not make gay jokes o...</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>Uncooked egg....😷 vomit #mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>uncooked egg vomit mkr</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>Im filled with joy.</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>im filled with joy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>@StephenAtWar IT IS CHARGING OKAY???</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>it is charging okay</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13458</th>\n",
       "      <td>@boxedariel @LostSailorNY Single men cannot ad...</td>\n",
       "      <td>gender</td>\n",
       "      <td>single men cannot adopt</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>@root_tim this is my work :)</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>this is my work</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38344 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type  \\\n",
       "44035  You so black and white trying to live like a n...          ethnicity   \n",
       "1317   @EurekAlertAAAS: Researchers push to import to...  not_cyberbullying   \n",
       "45165  @hermdiggz: “@tayyoung_: FUCK OBAMA, dumb ass ...          ethnicity   \n",
       "33724  ... I don't feel guilty for killing him, I jus...                age   \n",
       "10922  don't make rape jokes!!! don't make gay jokes!...             gender   \n",
       "...                                                  ...                ...   \n",
       "4990                        Uncooked egg....😷 vomit #mkr  not_cyberbullying   \n",
       "1956                                 Im filled with joy.  not_cyberbullying   \n",
       "7345                @StephenAtWar IT IS CHARGING OKAY???  not_cyberbullying   \n",
       "13458  @boxedariel @LostSailorNY Single men cannot ad...             gender   \n",
       "558                         @root_tim this is my work :)  not_cyberbullying   \n",
       "\n",
       "                                              text_clean  text_len  \n",
       "44035  you so black and white trying to live like a n...       319  \n",
       "1317   researchers push to import top antibullying pr...       273  \n",
       "45165  fuck obama dumb ass nigger this bitch ltthis w...       271  \n",
       "33724  i do not feel guilty for killing him i just fe...       270  \n",
       "10922  do not make rape jokes do not make gay jokes o...       220  \n",
       "...                                                  ...       ...  \n",
       "4990                              uncooked egg vomit mkr         4  \n",
       "1956                                  im filled with joy         4  \n",
       "7345                                 it is charging okay         4  \n",
       "13458                            single men cannot adopt         4  \n",
       "558                                      this is my work         4  \n",
       "\n",
       "[38344 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['text_len'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text_len'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = np.max(df['text_len'])\n",
    "max_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21241</th>\n",
       "      <td>And yet God was able to meet their needs using...</td>\n",
       "      <td>religion</td>\n",
       "      <td>and yet god was able to meet their needs using...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42078</th>\n",
       "      <td>FUCK TRAYVON and FUCK YOU! The lil thug nigger...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>fuck trayvon and fuck you the lil thug nigger ...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42651</th>\n",
       "      <td>Ayy Marco my dude I remember one of our first ...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>ayy marco my dude i remember one of our first ...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36235</th>\n",
       "      <td>tagged by older friend I bully 5 facts about m...</td>\n",
       "      <td>age</td>\n",
       "      <td>tagged by older friend i bully 5 facts about m...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34482</th>\n",
       "      <td>a lot of girls message me &amp;amp; ask me what i ...</td>\n",
       "      <td>age</td>\n",
       "      <td>a lot of girls message me amp ask me what i do...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>@hypatiadotca @GlennF but does it scale?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>but does it scale</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>@StephenAtWar IT IS CHARGING OKAY???</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>it is charging okay</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>- verdade @Mayara_Avez isso é #Bullying :/ rsrs '</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>verdade isso bullying rsrs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>i was borned with_______</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>i was borned with</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>So. Much. To. Hate.</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>so much to hate</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38333 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type  \\\n",
       "21241  And yet God was able to meet their needs using...           religion   \n",
       "42078  FUCK TRAYVON and FUCK YOU! The lil thug nigger...          ethnicity   \n",
       "42651  Ayy Marco my dude I remember one of our first ...          ethnicity   \n",
       "36235  tagged by older friend I bully 5 facts about m...                age   \n",
       "34482  a lot of girls message me &amp; ask me what i ...                age   \n",
       "...                                                  ...                ...   \n",
       "1828            @hypatiadotca @GlennF but does it scale?  not_cyberbullying   \n",
       "7345                @StephenAtWar IT IS CHARGING OKAY???  not_cyberbullying   \n",
       "1817   - verdade @Mayara_Avez isso é #Bullying :/ rsrs '  not_cyberbullying   \n",
       "1782                            i was borned with_______  not_cyberbullying   \n",
       "6505                                 So. Much. To. Hate.  not_cyberbullying   \n",
       "\n",
       "                                              text_clean  text_len  \n",
       "21241  and yet god was able to meet their needs using...        94  \n",
       "42078  fuck trayvon and fuck you the lil thug nigger ...        74  \n",
       "42651  ayy marco my dude i remember one of our first ...        66  \n",
       "36235  tagged by older friend i bully 5 facts about m...        65  \n",
       "34482  a lot of girls message me amp ask me what i do...        64  \n",
       "...                                                  ...       ...  \n",
       "1828                                   but does it scale         4  \n",
       "7345                                 it is charging okay         4  \n",
       "1817                          verdade isso bullying rsrs         4  \n",
       "1782                                   i was borned with         4  \n",
       "6505                                     so much to hate         4  \n",
       "\n",
       "[38333 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"text_len\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cyberbullying_type'] = df['cyberbullying_type'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_clean']\n",
    "y = df['cyberbullying_type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 5721],\n",
       "       [   1, 5703],\n",
       "       [   2, 5612],\n",
       "       [   3, 5410],\n",
       "       [   4, 5153]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
    "\n",
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1));\n",
    "train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text_clean', 'cyberbullying_type']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_os['text_clean'].values\n",
    "y_train = train_os['cyberbullying_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 5721],\n",
       "       [   1, 5721],\n",
       "       [   2, 5721],\n",
       "       [   3, 5721],\n",
       "       [   4, 5721]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "np.asarray((unique, counts)).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Bi-LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(column, seq_len):\n",
    "    ##Create vocabulary of words from column\n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    count_words = Counter(corpus)\n",
    "    sorted_words = count_words.most_common()\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "    ##Tokenize the columns text using the vocabulary\n",
    "    text_int = []\n",
    "    for text in column:\n",
    "        r = [vocab_to_int[word] for word in text.split()]\n",
    "        text_int.append(r)\n",
    "    ##Add padding to tokens\n",
    "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
    "    for i, review in enumerate(text_int):\n",
    "        if len(review) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(review)))\n",
    "            new = zeros + review\n",
    "        else:\n",
    "            new = review[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return sorted_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, tokenized_column = Tokenize(df[\"text_clean\"], max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the bully flushes on kd'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_clean\"].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     1,\n",
       "          55, 18671,    23, 18672])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_column[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "values = []\n",
    "for key, value in vocabulary[:20]:\n",
    "    keys.append(key)\n",
    "    values.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding by Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2vec_train_data = list(map(lambda x: x.split(), X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocabulary) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43042\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(vocabulary) + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (43042, 200)\n"
     ]
    }
   ],
   "source": [
    "#define empty embedding matrix\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "#fill the embedding matrix with the pre trained values from word2vec\n",
    "#    corresponding to word (string), token (number associated to the word)\n",
    "for word, token in vocabulary:\n",
    "    if word2vec_model.wv.__contains__(word):\n",
    "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Validation - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 5721],\n",
       "       [   1, 5721],\n",
       "       [   2, 5721],\n",
       "       [   3, 5721],\n",
       "       [   4, 5721]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tokenized_column\n",
    "y = df['cyberbullying_type'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
    "\n",
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "np.asarray((unique, counts)).T\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train))\n",
    "\n",
    "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train_os), torch.from_numpy(y_train_os))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch LSTM modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5 #We are dealing with a multiclass classification of 5 classes\n",
    "HIDDEN_DIM = 100 #number of neurons of the internal state (internal neural network in the LSTM)\n",
    "LSTM_LAYERS = 1 #Number of stacked LSTM layers\n",
    "\n",
    "LR = 3e-4 #Learning rate\n",
    "DROPOUT = 0.5 #LSTM Dropout\n",
    "BIDIRECTIONAL = True #Boolean value to choose if to use a bidirectional LSTM or not\n",
    "EPOCHS = 3 #Number of training epoch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Sentiment_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, bidirectional,batch_size, dropout):\n",
    "        super(BiLSTM_Sentiment_Classifier,self).__init__()\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        self.batch_size = x.size(0)\n",
    "        ##EMBEDDING LAYER\n",
    "        embedded = self.embedding(x)\n",
    "        #LSTM LAYERS\n",
    "        out, hidden = self.lstm(embedded, hidden)\n",
    "        #Extract only the hidden state from the last LSTM cell\n",
    "        out = out[:,-1,:]\n",
    "        #FULLY CONNECTED LAYERS\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Initialization of the LSTM hidden and cell states\n",
    "        h0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        c0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_Sentiment_Classifier(\n",
      "  (embedding): Embedding(43042, 200)\n",
      "  (lstm): LSTM(200, 100, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anas/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Sentiment_Classifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,NUM_CLASSES, LSTM_LAYERS,BIDIRECTIONAL, BATCH_SIZE, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "#Initialize embedding with the previously defined embedding matrix\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "#Allow the embedding matrix to be fined tuned to better adapt to out dataset and get higher accuracy\n",
    "model.embedding.weight.requires_grad=True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = 5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "early_stopping_patience = 4\n",
    "early_stopping_counter = 0\n",
    "\n",
    "valid_acc_max = 0 # Initialize best accuracy top 0\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    #lists to host the train and validation losses of every batch for each epoch\n",
    "    train_loss, valid_loss  = [], []\n",
    "    #lists to host the train and validation accuracy of every batch for each epoch\n",
    "    train_acc, valid_acc  = [], []\n",
    "\n",
    "    #lists to host the train and validation predictions of every batch for each epoch\n",
    "    y_train_list, y_val_list = [], []\n",
    "\n",
    "    #initalize number of total and correctly classified texts during training and validation\n",
    "    correct, correct_val = 0, 0\n",
    "    total, total_val = 0, 0\n",
    "    running_loss, running_loss_val = 0, 0\n",
    "\n",
    "\n",
    "    ####TRAINING LOOP####\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE) #load features and targets in device\n",
    "\n",
    "        h = model.init_hidden(labels.size(0))\n",
    "\n",
    "        model.zero_grad() #reset gradients \n",
    "\n",
    "        output, h = model(inputs,h) #get output and hidden states from LSTM network\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_train = torch.argmax(output, dim=1) #get tensor of predicted values on the training set\n",
    "        y_train_list.extend(y_pred_train.squeeze().tolist()) #transform tensor to list and the values to the list\n",
    "        \n",
    "        correct += torch.sum(y_pred_train==labels).item() #count correctly classified texts per batch\n",
    "        total += labels.size(0) #count total texts per batch\n",
    "\n",
    "    train_loss.append(running_loss / total_step)\n",
    "    train_acc.append(100 * correct / total)\n",
    "\n",
    "    ####VALIDATION LOOP####\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            val_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "\n",
    "            val_loss = criterion(output, labels)\n",
    "            running_loss_val += val_loss.item()\n",
    "\n",
    "            y_pred_val = torch.argmax(output, dim=1)\n",
    "            y_val_list.extend(y_pred_val.squeeze().tolist())\n",
    "\n",
    "            correct_val += torch.sum(y_pred_val==labels).item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "        valid_loss.append(running_loss_val / total_step_val)\n",
    "        valid_acc.append(100 * correct_val / total_val)\n",
    "\n",
    "    #Save model if validation accuracy increases\n",
    "    if np.mean(valid_acc) >= valid_acc_max:\n",
    "        torch.save(model.state_dict(), './state_dict.pt')\n",
    "        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n",
    "        valid_acc_max = np.mean(valid_acc)\n",
    "        early_stopping_counter=0 #reset counter if validation accuracy increases\n",
    "    else:\n",
    "        print(f'Epoch {e+1}:Validation accuracy did not increase')\n",
    "        early_stopping_counter+=1 #increase counter if validation accuracy does not increase\n",
    "        \n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', e+1)\n",
    "        break\n",
    "    \n",
    "    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n",
    "    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    test_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "    output, val_h = model(inputs, test_h)\n",
    "    y_pred_test = torch.argmax(output, dim=1)\n",
    "    y_pred_list.extend(y_pred_test.squeeze().tolist())\n",
    "    y_test_list.extend(labels.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report for Bi-LSTM :\\n', classification_report(y_test_list, y_pred_list, target_names=sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(y_test_list,y_pred_list,'PyTorch Bi-LSTM Sentiment Analysis\\nConfusion Matrix', sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
